{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from modules import models\n",
    "from modules import utility as uty\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU che usiamo e tempo per allenare + requirement che noi consigliamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uty.handle_GPUs(GPUs = '0', enable_GPU=1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed-up factor\n",
    "R = 20.0\n",
    "\n",
    "# decoder type (dec {0, 1, 2})\n",
    "dec = 2\n",
    "\n",
    "# loss type (L {0, 1, 2})\n",
    "L = 0\n",
    "\n",
    "# regularization weight (L1 10e-6)\n",
    "phi = 0\n",
    "\n",
    "# batch_size used to train MRI dataset is 16\n",
    "batch_size = 128\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "max_epochs_train = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ((dec==0 or dec==2) and L==0) or (dec==1 and (L==1 or L==2)), 'check for the right combinations of \"dec\" and \"L\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and split it in train, validation and test sets.  \n",
    "In the example the MNIST is loaded.\n",
    "\n",
    "The shape of the data must be compliant with:  \n",
    "data shape = (dataset size, width, length, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'MNIST'\n",
    "(xdata, _), (tdata, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "xdata = xdata[..., np.newaxis]/255\n",
    "vdata = tdata[:int(len(tdata)/2)][..., np.newaxis]/255\n",
    "tdata = tdata[int(len(tdata)/2):][..., np.newaxis]/255\n",
    "\n",
    "input_shape = np.shape(xdata[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dataset in use = ',dataset)\n",
    "print('train dataset size: ',np.shape(xdata))\n",
    "print('evaluation dataset size: ',np.shape(vdata))\n",
    "print('test dataset size: ',np.shape(tdata))\n",
    "input_shape = np.shape(xdata[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model class is loaded.  \n",
    "The depth attribute determines the depth of the U-NET. Because MNIST\n",
    "is a small dataset U-NET can coherentely be small and depth = 2 is used.  \n",
    "Default value is depth = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = uty.loupe_model(input_shape, R, dec, L, depth = 2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error (mae) is used to compute the error between the\n",
    "spatial images.   \n",
    "Mean Square Error (mse) is used to compute the error between the spatial arrays.\n",
    "\n",
    "Peak Signal to Noise Ration (PSNR) and Structural Similarity (SSIM) are not used \n",
    "as training losses but as additional monitors.\n",
    "\n",
    "Dec0 and Dec2 only takes one output (the reconstructed image),\n",
    "Dec1 takes two (the reconstructed image and \n",
    "the difference between the senses k-space and the encoded reconstructed image). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_PSNR(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.image.psnr(y_true, y_pred, 1))\n",
    "\n",
    "def metric_SSIM(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1))\n",
    "\n",
    "# regularization term used in L = {1, 2}\n",
    "def loss_norm_y(_, y_pred):\n",
    "    return tf.reduce_mean(tf.norm(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dec == 0 or dec == 2:\n",
    "    loss = 'mae'\n",
    "    loss_weights = [1]\n",
    "    metrics = [[metric_PSNR, metric_SSIM]]\n",
    "\n",
    "elif dec == 1:\n",
    "    loss = ['mae', loss_norm_y]\n",
    "    loss_weights = [1-phi, phi]\n",
    "    metrics = [[metric_PSNR, metric_SSIM], []]\n",
    "    \n",
    "    \n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss = loss,\n",
    "              loss_weights = loss_weights,\n",
    "              metrics = metrics,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the \"callbacks\" function in \"utility.py\" to include \n",
    "SaveModelOnCheckpoint and Tensorboard callabacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.callbacks.TerminateOnNaN object at 0x7fdba067b5e0>\n",
      "<keras.callbacks.ReduceLROnPlateau object at 0x7fdba06f5820>\n",
      "<keras.callbacks.EarlyStopping object at 0x7fdba06f5fd0>\n"
     ]
    }
   ],
   "source": [
    "callbacks = uty.callbacks()\n",
    "for i in callbacks:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_training = 1\n",
    "history = model.fit(xdata, \n",
    "                    xdata,\n",
    "                    initial_epoch = 0,\n",
    "                    epochs= max_epochs_train,\n",
    "                    validation_data = (vdata, vdata),\n",
    "                    batch_size = batch_size, \n",
    "                    callbacks = callbacks,\n",
    "                    verbose = verbose_training,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the tests, it is necessary to \n",
    "1. fix, and\n",
    "2. binarize  \n",
    "\n",
    "the mask.   \n",
    "This is necessary because during training the mask is randomly generated \n",
    "in accordance with a trainable probability mask.   \n",
    "This probability mask, to be trainable has to \n",
    "generate continuous values (not binary).\n",
    "\n",
    "\n",
    "We do this by calling \"change_setting( ..., setting = 'test')\"  \n",
    "By calling the same function with \" setting = 'train' \" the mask is unfixed and \n",
    "un-binarized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'test'\n",
    "verbose_settings = False\n",
    "\n",
    "model = uty.change_setting(model, setting = setting, \n",
    "                           verbose = verbose_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(tdata, tdata, batch_size = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dykstra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy from the paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_Dykstra = 50\n",
    "\n",
    "model_Dykstra = models.add_Dykstra_projection_to_model(\n",
    "        model, \n",
    "        iterations = iterations_Dykstra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'test'\n",
    "verbose_settings = False\n",
    "\n",
    "model_Dykstra = uty.change_setting(model_Dykstra, \n",
    "                                   setting = setting, \n",
    "                                   verbose = verbose_settings)\n",
    "\n",
    "model_Dykstra.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                      loss = loss,\n",
    "                      loss_weights = loss_weights,\n",
    "                      metrics = metrics,\n",
    "                     );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dopo avere aggiunto il "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Dykstra.evaluate(tdata, tdata, batch_size = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prob_mask = np.array(uty.read_probMask(model))[0,...,0]\n",
    "\n",
    "mask = prob_mask > 0.5\n",
    "mask_rot = uty._rotate_corners_mask(mask, plot = True, );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri = tdata[30][np.newaxis]\n",
    "\n",
    "model_encoded = tf.keras.Model(inputs = model.inputs, \n",
    "                               outputs = model.get_layer('ifft').output, )\n",
    "\n",
    "mri_encoded = np.array(model_encoded.predict(mri))\n",
    "\n",
    "mri_decoded = model.predict(mri)\n",
    "\n",
    "mri_Dykstra = np.array(model_Dykstra.predict(mri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(12)\n",
    "ax[0].imshow(mri[0, ..., 0], vmin = 0, vmax  = 1, cmap = 'gray')\n",
    "ax[0].title.set_text('ground truth')\n",
    "ax[1].imshow(mri_encoded[0, ..., 0], vmin = 0, vmax  = 1, cmap = 'gray')\n",
    "ax[1].title.set_text('encoded')\n",
    "ax[2].imshow(mri_Dykstra[0, ..., 0], vmin = 0, vmax  = 1, cmap = 'gray')\n",
    "ax[2].title.set_text('decoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More utilities (see utility.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are some functions in the utility.py that may be useful to the user:\n",
    "\n",
    "1. > uty.set_neurons_trainability(model, trainable, verbose)\n",
    "\n",
    "freezes (or unfreezes) the DECODER\n",
    "\n",
    "2. > uty.set_probMask_trainability(model, trainable, verbose)\n",
    "\n",
    "freezes (or unfreezes) the ENCODER (mask generator)\n",
    "\n",
    "3. > uty.set_slope_trainability(model, trainable, verbose)\n",
    "\n",
    "sets the trainability of the slope \"s\" of the mask generator\n",
    "\n",
    "4. > uty.set_mask_slope(model, slope, verbose)\n",
    "\n",
    "sets the value of the slope \"s\" of the mask generator\n",
    "\n",
    "5. > uty.set_mask_R(model, R, verbose)\n",
    "\n",
    "sets the acceleration factor \"R\" of the mask generator\n",
    "\n",
    "6. > uty.write_probMask(model, probMask, verbose)\n",
    "\n",
    "replaces the prob mask with a user-defined one\n",
    "\n",
    "7. > uty.copy_weights_by_matching_layer_name(model_dest, model, verbose)\n",
    "\n",
    "given two similar models (imagine \"model_dest\" has few layers adjointed to/removed from \"model\"), all the layers sharing the same name will share the same weights after the function is used\n",
    "\n",
    "8. > uty.handle_GPUs(GPUs, enable_GPU)\n",
    "\n",
    "selects the GPUs to use and activates the memory growth (for memory efficient training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
